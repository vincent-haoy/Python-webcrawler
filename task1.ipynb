{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "page_limit = 1000\n",
    "\n",
    "#Specify the initial page to crawl\n",
    "base_url = 'http://comp20008-jh.eng.unimelb.edu.au:9889/main/'\n",
    "seed_item = ''\n",
    "\n",
    "seed_url = base_url + seed_item\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "visited = {}; \n",
    "visited[seed_url] = True\n",
    "pages_visited = 1\n",
    "\n",
    "heads = soup.find_all('h1')\n",
    "f = open('task1.csv', 'w')\n",
    "f.write(seed_url)\n",
    "for head in heads:\n",
    "    f.write(','+str(head.text.strip()))\n",
    "f.write('\\n')\n",
    "\n",
    "#Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "#to_visit_relative = list(set(links) - set(seed_link))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "\n",
    "\n",
    "# Resolve to absolute urls\n",
    "booktitle = []\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "\n",
    "    \n",
    "#Find all outbound links on succsesor pages and explore each one \n",
    "while (to_visit):\n",
    "    # Impose a limit to avoid breaking the site \n",
    "    if pages_visited == page_limit :\n",
    "        break\n",
    "        \n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "    heads = soup.find_all('h1')\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    f.write(link)\n",
    "    heads = soup.find_all('h1')\n",
    "    for head in heads:\n",
    "        f.write(','+str(head.text.strip()))\n",
    "    f.write('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    new_links = soup.findAll('a')\n",
    "    for new_link in new_links :\n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited = pages_visited + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
