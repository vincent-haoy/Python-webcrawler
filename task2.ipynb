{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-6 7-6 (8-6) 6-4 \n",
      "5-7 7-5 7-6 \n",
      "7-6 (7-2) 6-4 \n",
      "6-4 7-5 \n",
      "7-6 (7-3) 6-3 \n",
      "7-5 6-4 \n",
      "7-5 6-3 \n",
      "6-2 6-2 \n",
      "6-3 6-1 \n",
      "4-6 6-3 7-6 \n",
      "7-6 (7-3) 6-3 \n",
      "7-5 6-3 \n",
      "6-3 6-1 \n",
      "6-7 6-4 6-4 \n",
      "7-5 6-1 \n",
      "6-4 7-5 \n",
      "7-5 6-4 \n",
      "6-3 6-7 (2-7) 7-6 (7-5) \n",
      "1-6 6-3 6-4 6-4 \n",
      "6-1 6-1 \n",
      "6-3 6-4 \n",
      "7-6 (7-4) 6-3 \n",
      "7-5 6-0 \n",
      "3-6 6-4 7-6 (7/5) \n",
      "6-4 6-4 \n",
      "7-5 6-3 \n",
      "3-6 7-6 (7-3) 7-6 (7-4) 6-1 \n",
      "6-4 6-1 \n",
      "6-3 6-2 1-6 3-6 10-8 \n",
      "7-5 6-3 \n",
      "7-6 (7-4) 6-3 \n",
      "6-1 6-2 \n",
      "7-5 7-6 (7-3) 6-1 \n",
      "7-5 6-3 3-6 6-3 \n",
      "6-4 7-6 (7-4) \n",
      "6-2 7-6 (7-1) 7-6 (7-5) \n",
      "6-0 6-3 6-2 \n",
      "6-1 7-5 \n",
      "6-2 7-6 (7-1) 7-6 (7-5) \n",
      "6-2 7-6 (7-1) 7-6 (7-5) \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "\n",
    "page_limit = 1000\n",
    "#json stuffs\n",
    "with open('tennis.json') as f:\n",
    "    Data = json.load(f)\n",
    "    names = [datum['name'] for datum in Data]\n",
    "\n",
    "#Specify the initial page to crawl\n",
    "base_url = 'http://comp20008-jh.eng.unimelb.edu.au:9889/main/'\n",
    "seed_items = ''\n",
    "patten = r'(\\(?.?\\d(-|/)\\d\\)?.?){2,}'\n",
    "#patten = r'(( ((\\(\\d-\\d\\))|(\\d-\\d))( |.|,|\\w)))'\n",
    "#patten = '\\w (((\\d-\\d)+(\\(\\d-\\d\\))+) )+\\w|.|,s' \n",
    "#the first page\n",
    "seed_url = base_url + seed_items\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "output = open(\"task2.csv\",\"w\")\n",
    "output.write(\"player,url,score,title\\n\")\n",
    "#if find the first page, set up the visiting condition\n",
    "visited = {}; \n",
    "visited[seed_url] = True\n",
    "pages_visited = 1\n",
    "\n",
    "#parse the first page\n",
    "body= soup.find('body')\n",
    "content = str(body.text.strip())\n",
    "content_length = len(content)\n",
    "for name in names:\n",
    "    break_flag = 0\n",
    "    name_length = len(name)\n",
    "    for x in range(content_length-name_length-1):\n",
    "        if name.lower() == content[x:x+name_length].lower():\n",
    "            print(name)\n",
    "            break_flag = 1\n",
    "            break;\n",
    "    if(break_flag):\n",
    "        break           \n",
    "#display(HTML(str(content)))\n",
    "#check valid function\n",
    "#Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "#to_visit_relative = list(set(links) - set(seed_link))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "def isvalid(score):\n",
    "    score = score.replace(\"/\",\"-\").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    each_rounds = score.split(\" \")\n",
    "    each_rounds.remove(\"\")\n",
    "    for each_round in each_rounds:\n",
    "        each_round = each_round.split(\"-\")\n",
    "        scorea = int(each_round[0])\n",
    "        scoreb = int(each_round[1])\n",
    "        if(scorea < 6 and scoreb < 6):\n",
    "            return False\n",
    "        if((scorea == 0 or scoreb == 0)and(scorea > 7 or scoreb>7 )):\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "\n",
    "# Resolve to absolute urls\n",
    "booktitle = []\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "\n",
    "    \n",
    "#Find all outbound links on succsesor pages and explore each one \n",
    "while (to_visit):\n",
    "    # Impose a limit to avoid breaking the site \n",
    "    if pages_visited == page_limit :\n",
    "        break\n",
    "        \n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    body= soup.find('body')\n",
    "    content = str(body.text.strip())\n",
    "    content_length = len(content)\n",
    "    #find the name and the score of that player\n",
    "    break_flag = 0\n",
    "    for x in range(content_length):\n",
    "        for name in names:\n",
    "            name_length = len(name)\n",
    "            if((name_length + x) > content_length):\n",
    "                break;\n",
    "            if name.lower() == content[x:x+name_length].lower():\n",
    "                score = re.search(patten,content[x:])\n",
    "                if score:\n",
    "                    score = score.group(0)\n",
    "                    purified_score = \"\"\n",
    "                    for element in score.split(\" \"):\n",
    "                        if element != '':\n",
    "                            element = element.replace(\".\",\"\").replace(\",\",\"\")\n",
    "                            purified_score = purified_score + element + \" \"\n",
    "                    if isvalid(purified_score):\n",
    "                        print(purified_score)\n",
    "                        output.write(name + \",\"+link+\",\"+purified_score+\",\")\n",
    "                        heads = soup.find_all('h1')\n",
    "                        for head in heads:\n",
    "                            output.write(\"\\\"\"+str(head.text.strip())+\"\\\"\")\n",
    "                        output.write(\"\\n\")                        \n",
    "                    purified_score = \"\"\n",
    "                    break_flag = 1\n",
    "                    break;\n",
    "        if(break_flag):\n",
    "            break    \n",
    "\n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    new_links = soup.findAll('a')\n",
    "    for new_link in new_links :\n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited = pages_visited + 1\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
